{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pySpark NLP and ML examples.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxE3e0nY2NQr",
        "outputId": "2cb1dcdf-db03-4f84-ea2f-9e1cb032cd53"
      },
      "source": [
        "import os\n",
        "# Find the latest version of spark 3.0  from http://www.apache.org/dist/spark/ and enter as the spark version\n",
        "# For example:\n",
        "# spark_version = 'spark-3.0.3'\n",
        "spark_version = 'spark-3.1.2'\n",
        "os.environ['SPARK_VERSION']=spark_version\n",
        "\n",
        "# Install Spark and Java\n",
        "!apt-get update\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz\n",
        "!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "# Set Environment Variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop2.7\"\n",
        "\n",
        "# Start a SparkSession\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:4 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Ign:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:11 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:13 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,431 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,365 kB]\n",
            "Hit:15 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:17 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,802 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,210 kB]\n",
            "Get:20 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [922 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,801 kB]\n",
            "Get:22 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 Packages [40.8 kB]\n",
            "Fetched 11.9 MB in 3s (3,548 kB/s)\n",
            "Reading package lists... Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fffPdY9szMm"
      },
      "source": [
        "# Start Spark session by importing the library and setting the spark variable to the code below\n",
        "# This creates a Spark application called \"DataFrameBasics.\"\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"DataFrameBasics\").getOrCreate()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mShel441vK4_",
        "outputId": "c706cbaf-ffea-4aa0-f26a-a1f57181f204"
      },
      "source": [
        "dataframe = spark.createDataFrame([(0, \"Here is our dataframe\"),\n",
        "                                   (1, \"We are making one from scratch\"),\n",
        "                                   (2, \"This will look very similar to a PandasDF\")], [\"id\", \"words\"])\n",
        "dataframe.show()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------------+\n",
            "| id|               words|\n",
            "+---+--------------------+\n",
            "|  0|Here is our dataf...|\n",
            "|  1|We are making one...|\n",
            "|  2|This will look ve...|\n",
            "+---+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPDXld73weqQ",
        "outputId": "9de98f9a-905e-4937-c06c-fce3bdae57ac"
      },
      "source": [
        "# Spark also lets us import data directly into a DataFrame. To do this, we import SparkFiles from the pyspark library that allows us to retrieve files.\n",
        "# The next three lines of code tell Spark to pull data from Amazon's Simple Storage Service (S3), a cloud-based data storage service. This boilerplate code can be used to read other public files hosted on Amazon's services.\n",
        "\n",
        "# Read in data from S3 Buckets\n",
        "from pyspark import SparkFiles\n",
        "url = \"https://s3.amazonaws.com/dataviz-curriculum/day_1/food.csv\"\n",
        "spark.sparkContext.addFile(url)\n",
        "df = spark.read.csv(SparkFiles.get(\"food.csv\"), sep=\",\", header=True)\n",
        "\n",
        "df.show()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+\n",
            "|   food|price|\n",
            "+-------+-----+\n",
            "|  pizza|    0|\n",
            "|  sushi|   12|\n",
            "|chinese|   10|\n",
            "+-------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ax3vlP4uww1T",
        "outputId": "2ee7a2fb-dd10-4092-c6e1-5e2fa883b11c"
      },
      "source": [
        "# Spark will infer the schema from the data, unless otherwise specified. We can check the schema by running the following code:\n",
        "\n",
        "# Print our schema\n",
        "df.printSchema()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- food: string (nullable = true)\n",
            " |-- price: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stObcXqjw3Vd",
        "outputId": "1110db39-8ac0-43f8-aafb-4bc066bb08c4"
      },
      "source": [
        "df.columns"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['food', 'price']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86TMlQnCw7D-",
        "outputId": "819b5e78-0690-461c-d292-b62cacad22b2"
      },
      "source": [
        "df.describe()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[summary: string, food: string, price: string]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3wkhv3axHQc"
      },
      "source": [
        "# we need to updayte the price to be a number\n",
        "# we can set our schema and then apply it to the data. We'll start by importing the different types of data with the following code\n",
        "\n",
        "# Import struct fields that we can use\n",
        "from pyspark.sql.types import StructField, StringType, IntegerType, StructType"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vB3zEqWtxM50",
        "outputId": "ad8a4c11-2518-4405-8c42-b5d45372c26c"
      },
      "source": [
        "# Next, create the schema by creating a StructType, which is one of Spark's complex types, like an array or map. The StructField will define the column name, the data type held, and a Boolean to define whether null values will be included or not:\n",
        "# Next we need to create the list of struct fields\n",
        "schema = [StructField(\"food\", StringType(), True), StructField(\"price\", IntegerType(), True),]\n",
        "schema"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[StructField(food,StringType,true), StructField(price,IntegerType,true)]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lzKN00zxSms",
        "outputId": "6e17d23e-b370-4e55-f190-68fa53ecf650"
      },
      "source": [
        "# Next, enter the code that will pass the schema just created as fields in a StructType. All this will be stored in a variable called final\n",
        "# Pass in our fields\n",
        "final = StructType(fields=schema)\n",
        "final"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StructType(List(StructField(food,StringType,true),StructField(price,IntegerType,true)))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Awr7uzLxbA0",
        "outputId": "e7f994cb-353f-4017-c2cc-11386f61ab9e"
      },
      "source": [
        "# Now that we have a predefined schema, we can read in the data again, only this time passing in our own schema. \n",
        "# Read our data with our new schema\n",
        "dataframe = spark.read.csv(SparkFiles.get(\"food.csv\"), schema=final, sep=\",\", header=True)\n",
        "dataframe.printSchema()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- food: string (nullable = true)\n",
            " |-- price: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YY8_jm2qx_c-",
        "outputId": "1b22710a-e0fc-4b29-d7fc-668e3a291cb9"
      },
      "source": [
        "# Add new column\n",
        "dataframe.withColumn('newprice', dataframe['price']).show()\n",
        "# Update column name\n",
        "dataframe.withColumnRenamed('price','newerprice').show()\n",
        "# Double the price\n",
        "dataframe.withColumn('doubleprice',dataframe['price']*2).show()\n",
        "# Add a dollar to the price\n",
        "dataframe.withColumn('add_one_dollar',dataframe['price']+1).show()\n",
        "# Half the price\n",
        "dataframe.withColumn('half_price',dataframe['price']/2).show()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+--------+\n",
            "|   food|price|newprice|\n",
            "+-------+-----+--------+\n",
            "|  pizza|    0|       0|\n",
            "|  sushi|   12|      12|\n",
            "|chinese|   10|      10|\n",
            "+-------+-----+--------+\n",
            "\n",
            "+-------+----------+\n",
            "|   food|newerprice|\n",
            "+-------+----------+\n",
            "|  pizza|         0|\n",
            "|  sushi|        12|\n",
            "|chinese|        10|\n",
            "+-------+----------+\n",
            "\n",
            "+-------+-----+-----------+\n",
            "|   food|price|doubleprice|\n",
            "+-------+-----+-----------+\n",
            "|  pizza|    0|          0|\n",
            "|  sushi|   12|         24|\n",
            "|chinese|   10|         20|\n",
            "+-------+-----+-----------+\n",
            "\n",
            "+-------+-----+--------------+\n",
            "|   food|price|add_one_dollar|\n",
            "+-------+-----+--------------+\n",
            "|  pizza|    0|             1|\n",
            "|  sushi|   12|            13|\n",
            "|chinese|   10|            11|\n",
            "+-------+-----+--------------+\n",
            "\n",
            "+-------+-----+----------+\n",
            "|   food|price|half_price|\n",
            "+-------+-----+----------+\n",
            "|  pizza|    0|       0.0|\n",
            "|  sushi|   12|       6.0|\n",
            "|chinese|   10|       5.0|\n",
            "+-------+-----+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9Yx5WzRyCsq"
      },
      "source": [
        "# In the last section, you might have noticed that when calling a column with Spark, no real results were shown. \n",
        "# As you might recall, Spark uses lazy evaluation. This means that Spark takes a list of instructions and formulates the best way to fulfill them, \n",
        "# and then waits until you tell Spark to complete them. The process of listing and reading the instructions is called transformation, \n",
        "# and your directive to complete them is called an action.\n",
        "\n",
        "# Start Spark session\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"DataFrameFunctions\").getOrCreate()\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxPhV_k1ycsN",
        "outputId": "9c7016ed-1a5a-4dbc-a909-4f55cf479dad"
      },
      "source": [
        "# Read in data from S3 Buckets\n",
        "from pyspark import SparkFiles\n",
        "url =\"https://s3.amazonaws.com/dataviz-curriculum/day_1/wine.csv\"\n",
        "spark.sparkContext.addFile(url)\n",
        "df = spark.read.csv(SparkFiles.get(\"wine.csv\"), sep=\",\", header=True)\n",
        "\n",
        "# Show DataFrame\n",
        "df.show()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------------------+--------------------+------+-----+------------------+--------------------+-----------------+------------------+--------------------+\n",
            "|country|         description|         designation|points|price|          province|            region_1|         region_2|           variety|              winery|\n",
            "+-------+--------------------+--------------------+------+-----+------------------+--------------------+-----------------+------------------+--------------------+\n",
            "|     US|This tremendous 1...|   Martha's Vineyard|    96|  235|        California|         Napa Valley|             Napa|Cabernet Sauvignon|               Heitz|\n",
            "|  Spain|Ripe aromas of fi...|Carodorum Selecci...|    96|  110|    Northern Spain|                Toro|             null|     Tinta de Toro|Bodega Carmen Rod...|\n",
            "|     US|Mac Watson honors...|Special Selected ...|    96|   90|        California|      Knights Valley|           Sonoma|   Sauvignon Blanc|            Macauley|\n",
            "|     US|This spent 20 mon...|             Reserve|    96|   65|            Oregon|   Willamette Valley|Willamette Valley|        Pinot Noir|               Ponzi|\n",
            "| France|This is the top w...|         La Br��lade|    95|   66|          Provence|              Bandol|             null|Provence red blend|Domaine de la B̩gude|\n",
            "|  Spain|Deep, dense and p...|           Numanthia|    95|   73|    Northern Spain|                Toro|             null|     Tinta de Toro|           Numanthia|\n",
            "|  Spain|Slightly gritty b...|          San Rom��n|    95|   65|    Northern Spain|                Toro|             null|     Tinta de Toro|            Maurodos|\n",
            "|  Spain|Lush cedary black...|Carodorum �_nico ...|    95|  110|    Northern Spain|                Toro|             null|     Tinta de Toro|Bodega Carmen Rod...|\n",
            "|     US|This re-named vin...|              Silice|    95|   65|            Oregon|  Chehalem Mountains|Willamette Valley|        Pinot Noir|           Bergstr̦m|\n",
            "|     US|The producer sour...|Gap's Crown Vineyard|    95|   60|        California|        Sonoma Coast|           Sonoma|        Pinot Noir|           Blue Farm|\n",
            "|  Italy|Elegance, complex...|  Ronco della Chiesa|    95|   80|Northeastern Italy|              Collio|             null|          Friulano|    Borgo del Tiglio|\n",
            "|     US|From 18-year-old ...|Estate Vineyard W...|    95|   48|            Oregon|        Ribbon Ridge|Willamette Valley|        Pinot Noir|Patricia Green Ce...|\n",
            "|     US|A standout even i...|      Weber Vineyard|    95|   48|            Oregon|        Dundee Hills|Willamette Valley|        Pinot Noir|Patricia Green Ce...|\n",
            "| France|This wine is in p...|Ch̢teau Montus Pr...|    95|   90|  Southwest France|             Madiran|             null|            Tannat|   Vignobles Brumont|\n",
            "|     US|With its sophisti...|      Grace Vineyard|    95|  185|            Oregon|        Dundee Hills|Willamette Valley|        Pinot Noir|      Domaine Serene|\n",
            "|     US|First made in 200...|              Sigrid|    95|   90|            Oregon|   Willamette Valley|Willamette Valley|        Chardonnay|           Bergstr̦m|\n",
            "|     US|This blockbuster,...|     Rainin Vineyard|    95|  325|        California|Diamond Mountain ...|             Napa|Cabernet Sauvignon|                Hall|\n",
            "|  Spain|Nicely oaked blac...|6 A̱os Reserva Pr...|    95|   80|    Northern Spain|    Ribera del Duero|             null|       Tempranillo|            Valduero|\n",
            "| France|Coming from a sev...|       Le Pigeonnier|    95|  290|  Southwest France|              Cahors|             null|            Malbec|  Ch̢teau Lagr̩zette|\n",
            "|     US|This fresh and li...|Gap's Crown Vineyard|    95|   75|        California|        Sonoma Coast|           Sonoma|        Pinot Noir|        Gary Farrell|\n",
            "+-------+--------------------+--------------------+------+-----+------------------+--------------------+-----------------+------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAAVnXmNyk3t",
        "outputId": "f11e4acc-2842-4aa5-a59c-4eba9be34942"
      },
      "source": [
        "# Transformations\n",
        "# with the data loaded in, let's perform some transformations\n",
        "# Order a DataFrame by ascending values\n",
        "#  All we're doing is telling Spark that we want this DataFrame to be organized in this particular way, and Spark says, \"Okay, got it—just let me know when you want me to do this.\"\n",
        "\n",
        "df.orderBy(df[\"points\"].desc())"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[country: string, description: string, designation: string, points: string, price: string, province: string, region_1: string, region_2: string, variety: string, winery: string]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wHZyUs9yx6d",
        "outputId": "55f5b4d0-bfb5-4e4a-897f-cfded923c83e"
      },
      "source": [
        "# Actions\n",
        "# Actions direct Spark to perform the computation instructions and return a result.\n",
        "# orderBy() and desc() are transformations telling Spark how to organize the data. Spark will read these transformations as instructions, but it won't act on them just yet.\n",
        "# show() is an action that gives the go-ahead for Spark to run all of those transformations and to produce a result.\n",
        "\n",
        "df.orderBy(df[\"points\"].desc()).show(5)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------------------+--------------------+------+-----+----------+-----------+--------+--------------------+--------------------+\n",
            "|country|         description|         designation|points|price|  province|   region_1|region_2|             variety|              winery|\n",
            "+-------+--------------------+--------------------+------+-----+----------+-----------+--------+--------------------+--------------------+\n",
            "|     US|This is an absolu...|           IX Estate|    99|  290|California|Napa Valley|    Napa|           Red Blend|              Colgin|\n",
            "| France|98-100 Barrel sam...|       Barrel sample|    99| null|  Bordeaux|   Pauillac|    null|Bordeaux-style Re...|Ch̢teau Pontet-Canet|\n",
            "|     US|There are incredi...|Elevation 1147 Es...|    99|  150|California|Napa Valley|    Napa|  Cabernet Sauvignon|        David Arthur|\n",
            "| France|A magnificent Cha...|Dom P̩rignon Oeno...|    99|  385| Champagne|  Champagne|    null|     Champagne Blend|     Mo��t & Chandon|\n",
            "|  Italy|Even better than ...|             Masseto|    99|  250|   Tuscany|    Toscana|    null|              Merlot|Tenuta dell'Ornel...|\n",
            "+-------+--------------------+--------------------+------+-----+----------+-----------+--------+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-N1-_qezVQw",
        "outputId": "a525eac0-003c-4b5e-c923-2afaa0e49f89"
      },
      "source": [
        "# More Functions\n",
        "\n",
        "#import functions\n",
        "\n",
        "#avg\n",
        "from pyspark.sql.functions import avg\n",
        "df.select(avg(\"points\")).show()\n",
        "\n",
        "#filter\n",
        "df.filter(\"price>20\").show(5)\n",
        "\n",
        "# filter and select certain columns; Both filter and select are separate transformations, and show is again the action.\n",
        "df.filter(\"price>20\").select([\"points\", \"country\"]).show(5)\n",
        "\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+\n",
            "|      avg(points)|\n",
            "+-----------------+\n",
            "|87.88834105383143|\n",
            "+-----------------+\n",
            "\n",
            "+-------+--------------------+--------------------+------+-----+--------------+-----------------+-----------------+------------------+--------------------+\n",
            "|country|         description|         designation|points|price|      province|         region_1|         region_2|           variety|              winery|\n",
            "+-------+--------------------+--------------------+------+-----+--------------+-----------------+-----------------+------------------+--------------------+\n",
            "|     US|This tremendous 1...|   Martha's Vineyard|    96|  235|    California|      Napa Valley|             Napa|Cabernet Sauvignon|               Heitz|\n",
            "|  Spain|Ripe aromas of fi...|Carodorum Selecci...|    96|  110|Northern Spain|             Toro|             null|     Tinta de Toro|Bodega Carmen Rod...|\n",
            "|     US|Mac Watson honors...|Special Selected ...|    96|   90|    California|   Knights Valley|           Sonoma|   Sauvignon Blanc|            Macauley|\n",
            "|     US|This spent 20 mon...|             Reserve|    96|   65|        Oregon|Willamette Valley|Willamette Valley|        Pinot Noir|               Ponzi|\n",
            "| France|This is the top w...|         La Br��lade|    95|   66|      Provence|           Bandol|             null|Provence red blend|Domaine de la B̩gude|\n",
            "+-------+--------------------+--------------------+------+-----+--------------+-----------------+-----------------+------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+------+-------+\n",
            "|points|country|\n",
            "+------+-------+\n",
            "|    96|     US|\n",
            "|    96|  Spain|\n",
            "|    96|     US|\n",
            "|    96|     US|\n",
            "|    95| France|\n",
            "+------+-------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z54UmbKWS8vw"
      },
      "source": [
        "# NLP - Tokenized data\n",
        "from pyspark.ml.feature import Tokenizer"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAqXR7m-WOzC",
        "outputId": "1e4c78ca-945b-4ec4-83f8-8bd06b1e3c6f"
      },
      "source": [
        "# We'll create a small DataFrame that will show the pre-tokenized data, using the following code\n",
        "#create sample dataframe\n",
        "dataframe = spark.createDataFrame([\n",
        "                                   (0, \"Spark is great.\"),\n",
        "                                   (1, \"We are learning Spark.\"),\n",
        "                                   (2, \"Spark is better than Hadoop no doubt.\")\n",
        "], [\"id\", \"sentence\"])\n",
        "dataframe.show()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------------+\n",
            "| id|            sentence|\n",
            "+---+--------------------+\n",
            "|  0|     Spark is great.|\n",
            "|  1|We are learning S...|\n",
            "|  2|Spark is better t...|\n",
            "+---+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJ1Crr4OXJ3V",
        "outputId": "3fa40a1a-8cdd-4257-baef-1628817c5b68"
      },
      "source": [
        "# The tokenizer function takes input and output parameters. The input passes the name of the column that we want to have tokenized, and the output takes the name that we want the column called.\n",
        "# Tokenize sentences\n",
        "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
        "tokenizer"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Tokenizer_ebd747bfdbf7"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QnClyZGqXckD",
        "outputId": "98e3fb2f-5c7c-4bcd-9fef-7a3ac4833652"
      },
      "source": [
        "# The tokenizer that we created uses a transform method that takes a DataFrame as input. This is a transformation, \n",
        "# so to reveal the results, we'll call show(truncate=False) as our action to display the results without shortening the output, as shown below:\n",
        "tokenizeddf = tokenizer.transform(dataframe)\n",
        "tokenizeddf.show(truncate=False)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------------------------------------+---------------------------------------------+\n",
            "|id |sentence                             |words                                        |\n",
            "+---+-------------------------------------+---------------------------------------------+\n",
            "|0  |Spark is great.                      |[spark, is, great.]                          |\n",
            "|1  |We are learning Spark.               |[we, are, learning, spark.]                  |\n",
            "|2  |Spark is better than Hadoop no doubt.|[spark, is, better, than, hadoop, no, doubt.]|\n",
            "+---+-------------------------------------+---------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPISXQD5Y5GR"
      },
      "source": [
        "# User-defined functions (UDFs) are functions created by the user to add custom output columns.\n",
        "# For the example below, we can create a function that will enhance our tokenizer by returning a word count for each line. \n",
        "# Start by creating a Python function that takes a list of words as its input, then returns the length of that list. \n",
        "\n",
        "# Create a function to return the length of a list\n",
        "def word_list_length(word_list):\n",
        "    return len(word_list)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lo8ScDs7ZFrS"
      },
      "source": [
        "#  we'll import the udf function, the col function to select a column to be passed into a function, and the type IntegerType that \n",
        "# will be used in our udf to define the data type of the output\n",
        "\n",
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.sql.types import IntegerType"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DX89YtqhZQR_"
      },
      "source": [
        "# Using the udf function, we can create our function to be passed in. The udf will take in the name of the function as a parameter \n",
        "# and the output data type, which is the IntegerType that we just imported.\n",
        "\n",
        "# Create a user defined function\n",
        "count_tokens = udf(word_list_length, IntegerType())"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-avsLacEZbYZ",
        "outputId": "f5bac599-805c-4d1e-8830-2a3fcd0f9aff"
      },
      "source": [
        "# Now we can redo the tokenizer process. Only this time, after the DataFrame has outputted the tokenized values, \n",
        "# we can use our own created function to return the number of tokens created. This will give us another data point to use in the future, if needed.\n",
        "\n",
        "# create our tokenizer\n",
        "tokenizer2 = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
        "tokenizeddf2 = tokenizer2.transform(dataframe)\n",
        "\n",
        "#select the needed columns and don't truncate the results\n",
        "tokenizeddf2.withColumn(\"tokens\", count_tokens(col(\"words\"))).show(truncate=False)\n",
        "\n",
        "\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------------------------------------+---------------------------------------------+------+\n",
            "|id |sentence                             |words                                        |tokens|\n",
            "+---+-------------------------------------+---------------------------------------------+------+\n",
            "|0  |Spark is great.                      |[spark, is, great.]                          |3     |\n",
            "|1  |We are learning Spark.               |[we, are, learning, spark.]                  |4     |\n",
            "|2  |Spark is better than Hadoop no doubt.|[spark, is, better, than, hadoop, no, doubt.]|7     |\n",
            "+---+-------------------------------------+---------------------------------------------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVOKNmeUawCm",
        "outputId": "684ea55e-0b65-4141-820b-1ebb316a926d"
      },
      "source": [
        "# Stop Words\n",
        "# Stop words are words that have little or no linguistic value in NLP. \n",
        "# Removing these words from the data can improve the accuracy of the language model because it removes inessential words.\n",
        "# Let's take a look at the stop word removal code.\n",
        "\n",
        "# create a dataframe\n",
        "sentenceData = spark.createDataFrame([\n",
        "                                      (0, [\"big\", \"data\", \"is\", \"super\", \"powerful\"]),\n",
        "                                      (1, [\"this\", \"is\", \"going\", \"to\", \"be\", \"epic\"])\n",
        "], [\"id\", \"raw\"])\n",
        "sentenceData.show(truncate=False)\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------------------------+\n",
            "|id |raw                             |\n",
            "+---+--------------------------------+\n",
            "|0  |[big, data, is, super, powerful]|\n",
            "|1  |[this, is, going, to, be, epic] |\n",
            "+---+--------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xqkel5v3eXO_"
      },
      "source": [
        "# Import stop words library\n",
        "from pyspark.ml.feature import StopWordsRemover"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_05RFo1WeexE"
      },
      "source": [
        "# Run the Remover\n",
        "remover = StopWordsRemover(inputCol=\"raw\", outputCol=\"filtered\")"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0D_VV9FelrZ",
        "outputId": "d1910635-4b0e-43b1-f91c-ea844524813d"
      },
      "source": [
        "# transform and show data\n",
        "remover.transform(sentenceData).show(truncate=False)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------------------------+----------------------------+\n",
            "|id |raw                             |filtered                    |\n",
            "+---+--------------------------------+----------------------------+\n",
            "|0  |[big, data, is, super, powerful]|[big, data, super, powerful]|\n",
            "|1  |[this, is, going, to, be, epic] |[going, epic]               |\n",
            "+---+--------------------------------+----------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1ScxtxKfk3F"
      },
      "source": [
        "# Term Frequency-Inverse Document Frequency Weight\n",
        "# Term frequency (TF) measures the frequency of a word occurring in a document\n",
        "#  inverse document frequency (IDF) measures the significance of a word across a set of documents\n",
        "\n",
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StopWordsRemover"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHnOTyaIrgqt",
        "outputId": "e90a3ab5-46ae-48f8-db5f-d39f1bb82c70"
      },
      "source": [
        "# This is a dataset of tweets from people directed at an airline. We'll load in the data the same way we did when working with DataFrames earlier\n",
        "# Read in data from S3 Buckets\n",
        "from pyspark import SparkFiles\n",
        "url =\"https://s3.amazonaws.com/dataviz-curriculum/day_2/airlines.csv\"\n",
        "spark.sparkContext.addFile(url)\n",
        "df = spark.read.csv(SparkFiles.get(\"airlines.csv\"), sep=\",\", header=True)\n",
        "\n",
        "# Show DataFrame\n",
        "df.show()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|      Airline Tweets|\n",
            "+--------------------+\n",
            "|@VirginAmerica pl...|\n",
            "|@VirginAmerica se...|\n",
            "|@VirginAmerica do...|\n",
            "|@VirginAmerica Ar...|\n",
            "|@VirginAmerica aw...|\n",
            "+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JAyfXmHPrvPe",
        "outputId": "bae67f07-87d0-40a3-a187-5593c6e73760"
      },
      "source": [
        "# Tokenize DataFrame\n",
        "tokened = Tokenizer(inputCol=\"Airline Tweets\", outputCol=\"words\")\n",
        "tokened_transformed = tokened.transform(df)\n",
        "tokened_transformed.show()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+\n",
            "|      Airline Tweets|               words|\n",
            "+--------------------+--------------------+\n",
            "|@VirginAmerica pl...|[@virginamerica, ...|\n",
            "|@VirginAmerica se...|[@virginamerica, ...|\n",
            "|@VirginAmerica do...|[@virginamerica, ...|\n",
            "|@VirginAmerica Ar...|[@virginamerica, ...|\n",
            "|@VirginAmerica aw...|[@virginamerica, ...|\n",
            "+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NarqPIkErxuF",
        "outputId": "8a9dd9cb-6cb8-40e3-da2c-ca6e6e310ed9"
      },
      "source": [
        "# Remove stop words\n",
        "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
        "removed_frame = remover.transform(tokened_transformed)\n",
        "removed_frame.show(truncate=False)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n",
            "|Airline Tweets                                                                                                                         |words                                                                                                                                                          |filtered                                                                                       |\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n",
            "|@VirginAmerica plus you've added commercials to the experience... tacky.                                                               |[@virginamerica, plus, you've, added, commercials, to, the, experience..., tacky.]                                                                             |[@virginamerica, plus, added, commercials, experience..., tacky.]                              |\n",
            "|@VirginAmerica seriously would pay $30 a flight for seats that didn't have this playing. it's really the only bad thing about flying VA|[@virginamerica, seriously, would, pay, $30, a, flight, for, seats, that, didn't, have, this, playing., it's, really, the, only, bad, thing, about, flying, va]|[@virginamerica, seriously, pay, $30, flight, seats, playing., really, bad, thing, flying, va] |\n",
            "|@VirginAmerica do you miss me? Don't worry we'll be together very soon.                                                                |[@virginamerica, do, you, miss, me?, don't, worry, we'll, be, together, very, soon.]                                                                           |[@virginamerica, miss, me?, worry, together, soon.]                                            |\n",
            "|@VirginAmerica Are the hours of operation for the Club at SFO that are posted online current?                                          |[@virginamerica, are, the, hours, of, operation, for, the, club, at, sfo, that, are, posted, online, current?]                                                 |[@virginamerica, hours, operation, club, sfo, posted, online, current?]                        |\n",
            "|@VirginAmerica awaiting my return phone call, just would prefer to use your online self-service option :(                              |[@virginamerica, awaiting, my, return, phone, call,, just, would, prefer, to, use, your, online, self-service, option, :(]                                     |[@virginamerica, awaiting, return, phone, call,, prefer, use, online, self-service, option, :(]|\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9H1-EOQr2UO",
        "outputId": "f67e716c-e742-4c9a-8994-46e9a7493008"
      },
      "source": [
        "# The HashingTF function takes an argument for an input column, an output column, and a numFeature parameter, \n",
        "# which specifies the number of buckets for the split words. This number must be higher than the number of unique words. \n",
        "# By default, the value is 2^18^or 262,144. The power of two should be used so that indexes are evenly mapped. Here we supply \n",
        "# the numFeatures argument with its default value for demonstration. This argument can normally be left out and will use the same value by default.\n",
        "\n",
        "# RUN THE HASHING TERM FREQUENCY\n",
        "hashing = HashingTF(inputCol=\"filtered\", outputCol=\"hashedValues\", numFeatures=pow(2,18))\n",
        "\n",
        "#TRANSFORM INTO A DATAFRAME; scroll right to see the hashed result;\n",
        "hashed_df = hashing.transform(removed_frame) \n",
        "hashed_df.show(truncate=False)\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|Airline Tweets                                                                                                                         |words                                                                                                                                                          |filtered                                                                                       |hashedValues                                                                                                                               |\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|@VirginAmerica plus you've added commercials to the experience... tacky.                                                               |[@virginamerica, plus, you've, added, commercials, to, the, experience..., tacky.]                                                                             |[@virginamerica, plus, added, commercials, experience..., tacky.]                              |(262144,[1419,99916,168274,171322,186669,256498],[1.0,1.0,1.0,1.0,1.0,1.0])                                                                |\n",
            "|@VirginAmerica seriously would pay $30 a flight for seats that didn't have this playing. it's really the only bad thing about flying VA|[@virginamerica, seriously, would, pay, $30, a, flight, for, seats, that, didn't, have, this, playing., it's, really, the, only, bad, thing, about, flying, va]|[@virginamerica, seriously, pay, $30, flight, seats, playing., really, bad, thing, flying, va] |(262144,[30053,44911,70065,94512,99549,145380,166947,171322,178915,229264,237593,239713],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
            "|@VirginAmerica do you miss me? Don't worry we'll be together very soon.                                                                |[@virginamerica, do, you, miss, me?, don't, worry, we'll, be, together, very, soon.]                                                                           |[@virginamerica, miss, me?, worry, together, soon.]                                            |(262144,[107065,117975,147224,171322,200547,232735],[1.0,1.0,1.0,1.0,1.0,1.0])                                                             |\n",
            "|@VirginAmerica Are the hours of operation for the Club at SFO that are posted online current?                                          |[@virginamerica, are, the, hours, of, operation, for, the, club, at, sfo, that, are, posted, online, current?]                                                 |[@virginamerica, hours, operation, club, sfo, posted, online, current?]                        |(262144,[9641,50671,83962,96266,171322,181964,192171,220194],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                            |\n",
            "|@VirginAmerica awaiting my return phone call, just would prefer to use your online self-service option :(                              |[@virginamerica, awaiting, my, return, phone, call,, just, would, prefer, to, use, your, online, self-service, option, :(]                                     |[@virginamerica, awaiting, return, phone, call,, prefer, use, online, self-service, option, :(]|(262144,[6122,50509,50671,67607,98717,121947,128077,171322,225517,234877,261675],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])            |\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "liGSp6nxtEsv",
        "outputId": "a85f3ad4-5ad6-4faa-bf0a-650c8c38ac48"
      },
      "source": [
        "# With our words successfully converted to numbers, we can plug it all into an IDFModel, \n",
        "# which will scale the values while down-weighting based on document frequency.\n",
        "\n",
        "# fit the IDF to the dataset \n",
        "idf = IDF(inputCol=\"hashedValues\", outputCol=\"features\")\n",
        "idfModel = idf.fit(hashed_df)\n",
        "rescaledData = idfModel.transform(hashed_df)\n",
        "\n",
        "#display the DF, scroll to the right to see the new output column\n",
        "rescaledData.select(\"words\", \"features\").show(truncate=False)\n",
        "\n",
        "# Remember that computers can't just read text and analyze it. With the process we have just completed, \n",
        "# we have now given values from raw text that a computer can work with. \n",
        "# The next section will put all of this together and show how we can use text data to determine the accuracy of the corresponding rating."
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|words                                                                                                                                                          |features                                                                                                                                                                                                                                                                                                        |\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|[@virginamerica, plus, you've, added, commercials, to, the, experience..., tacky.]                                                                             |(262144,[1419,99916,168274,171322,186669,256498],[1.0986122886681098,1.0986122886681098,1.0986122886681098,0.0,1.0986122886681098,1.0986122886681098])                                                                                                                                                          |\n",
            "|[@virginamerica, seriously, would, pay, $30, a, flight, for, seats, that, didn't, have, this, playing., it's, really, the, only, bad, thing, about, flying, va]|(262144,[30053,44911,70065,94512,99549,145380,166947,171322,178915,229264,237593,239713],[1.0986122886681098,1.0986122886681098,1.0986122886681098,1.0986122886681098,1.0986122886681098,1.0986122886681098,1.0986122886681098,0.0,1.0986122886681098,1.0986122886681098,1.0986122886681098,1.0986122886681098])|\n",
            "|[@virginamerica, do, you, miss, me?, don't, worry, we'll, be, together, very, soon.]                                                                           |(262144,[107065,117975,147224,171322,200547,232735],[1.0986122886681098,1.0986122886681098,1.0986122886681098,0.0,1.0986122886681098,1.0986122886681098])                                                                                                                                                       |\n",
            "|[@virginamerica, are, the, hours, of, operation, for, the, club, at, sfo, that, are, posted, online, current?]                                                 |(262144,[9641,50671,83962,96266,171322,181964,192171,220194],[1.0986122886681098,0.6931471805599453,1.0986122886681098,1.0986122886681098,0.0,1.0986122886681098,1.0986122886681098,1.0986122886681098])                                                                                                        |\n",
            "|[@virginamerica, awaiting, my, return, phone, call,, just, would, prefer, to, use, your, online, self-service, option, :(]                                     |(262144,[6122,50509,50671,67607,98717,121947,128077,171322,225517,234877,261675],[1.0986122886681098,1.0986122886681098,0.6931471805599453,1.0986122886681098,1.0986122886681098,1.0986122886681098,1.0986122886681098,0.0,1.0986122886681098,1.0986122886681098,1.0986122886681098])                           |\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArOSAtfxuxVf",
        "outputId": "59683246-edf7-4bb7-bd78-ef212877d474"
      },
      "source": [
        "# Set Up the Pipeline\n",
        "# We'll put the pipeline to use by employing a sample Yelp dataset. \n",
        "# This will allow you to practice with a smaller set of reviews using similar data that you'll be working on for your client.\n",
        "\n",
        "# Read in data from S3 Buckets\n",
        "from pyspark import SparkFiles\n",
        "url =\"https://s3.amazonaws.com/dataviz-curriculum/day_2/yelp_reviews.csv\"\n",
        "spark.sparkContext.addFile(url)\n",
        "df = spark.read.csv(SparkFiles.get(\"yelp_reviews.csv\"), sep=\",\", header=True)\n",
        "\n",
        "# Show DataFrame\n",
        "df.show()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------------------+\n",
            "|   class|                text|\n",
            "+--------+--------------------+\n",
            "|positive|Wow... Loved this...|\n",
            "|negative|  Crust is not good.|\n",
            "|negative|Not tasty and the...|\n",
            "|positive|Stopped by during...|\n",
            "|positive|The selection on ...|\n",
            "|negative|Now I am getting ...|\n",
            "|negative|Honeslty it didn'...|\n",
            "|negative|The potatoes were...|\n",
            "|positive|The fries were gr...|\n",
            "|positive|      A great touch.|\n",
            "|positive|Service was very ...|\n",
            "|negative|  Would not go back.|\n",
            "|negative|The cashier had n...|\n",
            "|positive|I tried the Cape ...|\n",
            "|negative|I was disgusted b...|\n",
            "|negative|I was shocked bec...|\n",
            "|positive| Highly recommended.|\n",
            "|negative|Waitress was a li...|\n",
            "|negative|This place is not...|\n",
            "|negative|did not like at all.|\n",
            "+--------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENHpiqR0vNbk"
      },
      "source": [
        "#  import all the functions that will be used in our NLP process, or pipeline\n",
        "# Import functions\n",
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, StringIndexer"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sr2yZ93uvRiX",
        "outputId": "e77dc406-d8e8-465c-a4b0-ebf6b456800e"
      },
      "source": [
        "# Next, create a new column that uses the lengthfunction to create a future feature with the length of each row. \n",
        "from pyspark.sql.functions import length\n",
        "# Create a length column to be used as a future feature\n",
        "data_df = df.withColumn('length', length(df['text']))\n",
        "data_df.show()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------------------+------+\n",
            "|   class|                text|length|\n",
            "+--------+--------------------+------+\n",
            "|positive|Wow... Loved this...|    24|\n",
            "|negative|  Crust is not good.|    18|\n",
            "|negative|Not tasty and the...|    41|\n",
            "|positive|Stopped by during...|    87|\n",
            "|positive|The selection on ...|    59|\n",
            "|negative|Now I am getting ...|    46|\n",
            "|negative|Honeslty it didn'...|    37|\n",
            "|negative|The potatoes were...|   111|\n",
            "|positive|The fries were gr...|    25|\n",
            "|positive|      A great touch.|    14|\n",
            "|positive|Service was very ...|    24|\n",
            "|negative|  Would not go back.|    18|\n",
            "|negative|The cashier had n...|    99|\n",
            "|positive|I tried the Cape ...|    59|\n",
            "|negative|I was disgusted b...|    62|\n",
            "|negative|I was shocked bec...|    50|\n",
            "|positive| Highly recommended.|    19|\n",
            "|negative|Waitress was a li...|    38|\n",
            "|negative|This place is not...|    51|\n",
            "|negative|did not like at all.|    20|\n",
            "+--------+--------------------+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHPri1jovd3t"
      },
      "source": [
        "# Create all the features to the data set\n",
        "pos_neg_to_num = StringIndexer(inputCol='class',outputCol='label')\n",
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"token_text\")\n",
        "stopremove = StopWordsRemover(inputCol='token_text',outputCol='stop_tokens')\n",
        "hashingTF = HashingTF(inputCol=\"stop_tokens\", outputCol='hash_token')\n",
        "idf = IDF(inputCol='hash_token', outputCol='idf_token')"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1w8AiyJvrLr"
      },
      "source": [
        "we'll create all the transformations to be applied in our pipeline. Note that the StringIndexer encodes a string column to a column of table indexes. Here we are working with positive and negative game reviews, which will be converted to 0 and 1. This will form our labels, which we'll delve into in the ML unit. The label is what we're trying to predict: will the review's given text let us know if it was positive or negative?\n",
        "\n",
        "Also note that we don't need to run all of these completely as we did before. By creating all the functions now, we can then use them all in the pipeline later.\n",
        "\n",
        "We'll create a feature vector containing the output from the IDFModel (the last stage in the pipeline) and the length. This will combine all the raw features to train the ML model that we'll be using. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niWi__jEv4yk"
      },
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.linalg import Vector\n",
        "# Create feature vectors\n",
        "clean_up = VectorAssembler(inputCols=['idf_token', 'length'], outputCol='features')"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fn2Gg1HowK1A"
      },
      "source": [
        "Now it's time to create our pipeline, the easiest step. We'll import the pipeline from pyspark.ml, and then store a list of the stages created earlier. It's important to list the stages in the order they need to be executed. As we mentioned before, the output from one stage will then be passed off to another stage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xc8CU9TcwL6K"
      },
      "source": [
        "# Create and run a data processing Pipeline\n",
        "from pyspark.ml import Pipeline\n",
        "data_prep_pipeline = Pipeline(stages=[pos_neg_to_num, tokenizer, stopremove, hashingTF, idf, clean_up])"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQ_Bm066wUSD"
      },
      "source": [
        "All the stages of the pipeline have been set up. You may have noticed that each individual step didn't need to be set up, which is the perk of setting up the pipeline! Now your data is ready to be run through the pipeline. Then we can run it through a machine learning model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6IqVj4OwiM0"
      },
      "source": [
        "# Run the Model\n",
        "# After our pipeline has been set up, we'll fit the outcome with our original DataFrame and transform it.\n",
        "\n",
        "# Fit and transform the pipeline\n",
        "cleaner = data_prep_pipeline.fit(data_df)\n",
        "cleaned = cleaner.transform(data_df)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUUPZmBcwyCJ"
      },
      "source": [
        "our labels and features that we created early on in the process are numerical representations of positive and negative reviews. The features will be used in our model and predict whether a given review will be positive or negative. These features are the result of all the work we have been doing with the pipeline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYUs4YRNwy5Q",
        "outputId": "94a67502-0bb3-4be4-867d-25be3c443a43"
      },
      "source": [
        "# show label and resulting features\n",
        "cleaned.select([\"label\", \"features\"]).show()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------------+\n",
            "|label|            features|\n",
            "+-----+--------------------+\n",
            "|  1.0|(262145,[177414,2...|\n",
            "|  0.0|(262145,[49815,23...|\n",
            "|  0.0|(262145,[109649,1...|\n",
            "|  1.0|(262145,[53101,68...|\n",
            "|  1.0|(262145,[15370,77...|\n",
            "|  0.0|(262145,[98142,13...|\n",
            "|  0.0|(262145,[59172,22...|\n",
            "|  0.0|(262145,[63420,85...|\n",
            "|  1.0|(262145,[53777,17...|\n",
            "|  1.0|(262145,[221827,2...|\n",
            "|  1.0|(262145,[43756,22...|\n",
            "|  0.0|(262145,[127310,1...|\n",
            "|  0.0|(262145,[407,3153...|\n",
            "|  1.0|(262145,[18098,93...|\n",
            "|  0.0|(262145,[23071,12...|\n",
            "|  0.0|(262145,[129941,1...|\n",
            "|  1.0|(262145,[19633,21...|\n",
            "|  0.0|(262145,[27707,65...|\n",
            "|  0.0|(262145,[20891,27...|\n",
            "|  0.0|(262145,[8287,208...|\n",
            "+-----+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTeTO5qWxPiX"
      },
      "source": [
        "Now let's run our ML model on the data. One of the basics of ML is that data gets broken into training data and testing data. Training data is the data that will be passed to our NLP model that will train our model to predict results. The testing data is used to test our predictions. We can do this with the randomSplit method, which takes in a list of the percent of data we want split into each group. Standard conventions use 70% with training and 30% with testing.\n",
        "\n",
        "To split the data into a training set and a testing set, run the following code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTf6_HQHxRjN"
      },
      "source": [
        "# Break data down into a training set and a testing set\n",
        "training, testing = cleaned.randomSplit([0.7, 0.3], 21)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIJEGu1txlo5"
      },
      "source": [
        "The array supplied to randomSplit is the percentage of the data that will be broken into training and testing respectively. So 70% to training and 30% to testing. The second number supplied is called a seed. The seed number here, 21, is arbitrary. But as long as the same seed is used, the result will be the same each time. Using a seed number ensures reproducible results.\n",
        "\n",
        "The ML model we'll use is Naive Bayes, which we'll import and then fit the model using the training dataset. Naive Bayes is a group of classifier algorithms based on Bayes' theorem. Bayes theorem provides a way to determine the probability of an event based on new conditions or information that might be related to the event."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4CqUepKxXAl"
      },
      "source": [
        "from pyspark.ml.classification import NaiveBayes\n",
        "# Create a Naive Bayes model and fit training data\n",
        "nb = NaiveBayes()\n",
        "predictor = nb.fit(training)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nM7XkqXnyOYa",
        "outputId": "d024db89-de0f-4a88-ea2f-fe88d554ea9f"
      },
      "source": [
        "# Once the model has been trained, we'll transform the model with our testing data. \n",
        "# transform the model with the testing data\n",
        "test_results = predictor.transform(testing)\n",
        "test_results.show(5)\n",
        "\n",
        "# scroll all the way to the right to view the prediction column"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------------------+------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
            "|   class|                text|length|label|          token_text|         stop_tokens|          hash_token|           idf_token|            features|       rawPrediction|         probability|prediction|\n",
            "+--------+--------------------+------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
            "|negative|\"The burger... I ...|    86|  0.0|[\"the, burger...,...|[\"the, burger...,...|(262144,[20298,21...|(262144,[20298,21...|(262145,[20298,21...|[-820.60780566975...|[0.99999999999995...|       0.0|\n",
            "|negative|              #NAME?|     6|  0.0|            [#name?]|            [#name?]|(262144,[197050],...|(262144,[197050],...|(262145,[197050,2...|[-73.489435340867...|[0.07515735596910...|       1.0|\n",
            "|negative|After I pulled up...|    83|  0.0|[after, i, pulled...|[pulled, car, wai...|(262144,[65645,71...|(262144,[65645,71...|(262145,[65645,71...|[-620.40646705112...|[1.0,1.9205984091...|       0.0|\n",
            "|negative|Also, I feel like...|    58|  0.0|[also,, i, feel, ...|[also,, feel, lik...|(262144,[61899,66...|(262144,[61899,66...|(262145,[61899,66...|[-528.59562125515...|[0.99999999994873...|       0.0|\n",
            "|negative|Anyway, I do not ...|    44|  0.0|[anyway,, i, do, ...|[anyway,, think, ...|(262144,[132270,1...|(262144,[132270,1...|(262145,[132270,1...|[-334.09599709326...|[0.99999999994185...|       0.0|\n",
            "+--------+--------------------+------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpaNR9Z1zJbp"
      },
      "source": [
        "This prediction column will indicate with a 1.0 if the model thinks this review is negative and 0.0 if it thinks it's positive. Future data sets can now be run with this model and determine whether a review was positive or negative without having already supplied in the data.\n",
        "\n",
        "How useful is this model? Should we just blindly trust that it will be right every time? There is one last step in the process to answer these questions.\n",
        "\n",
        "It's often not enough to simply train and use a machine learning model for predictions without knowing how well the model performs at its prediction task. The last step is to import the BinaryClassificationEvaluator, which will display how accurate our model is in determining if a review with be positive or negative based solely on the text within a review.\n",
        "\n",
        "The BinaryClassificationEvaluator uses two arguments, labelCol and rawPredictionCol. The labelCol takes the labels which were the result of using StringIndexer to convert our positive and negative strings to integers. The rawPredictionCol takes in numerical predictions from the output of running the Naive Bayes model.\n",
        "\n",
        "The performance of a model can be measured based on the difference between its predicted values and actual values. This is what the BinaryClassificationEvaluator does. We will dive more into accuracy, precision, and sensitivity when we get to Machine Learning. Run the following code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WrAO_ucAzLf0",
        "outputId": "edc9a5a4-05eb-4646-9d5f-31883e399108"
      },
      "source": [
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "acc_eval = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\")\n",
        "acc = acc_eval.evaluate(test_results)\n",
        "print(\"Accuracy of model at predicting reviews was: %f\" % acc)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of model at predicting reviews was: 0.700298\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tw0hparp0svC"
      },
      "source": [
        "The accuracy of the model isn't perfect, but it's not too low either: 0.700298. Machine learning isn't a guarantee, and tweaking our models as well as the data used is part of the process. One of the ways to do this is to add more data; when you keep adding data, eventually you grow from your local storage to something much larger—thus leading to big data!"
      ]
    }
  ]
}